\documentclass{article}
\usepackage{blindtext}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{parskip}

\usepackage[a4paper]{geometry}
\usepackage[toc,page]{appendix}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=cyan,
}

\title{MIT 6.006 Introduction to Algorithms}
\author{Sebastian Nyberg}
\date{\today}

\begin{document}

\setcounter{section}{-1}

\maketitle
\tableofcontents
\clearpage

% Table options
\renewcommand{\arraystretch}{1.} % better line spacing

\section{Overview}

\begin{itemize}
  \item \href{https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/index.htm}{Course Website}
\end{itemize}

\section{Algorithmic thinking, asymptotic complexity, peak finding}

Links:

\begin{itemize}
  \item \href{https://www.youtube.com/watch?v=HtSuA80QTyo}{Lecture}
  \item \href{https://www.youtube.com/watch?v=P7frcB_-g4w&}{Recitation}
\end{itemize}

\noindent
This lecture is an introduction to algorithms in general. 

\subsection{Asymptotic complexity}

Time complexities:

\begin{itemize}
  \item Upper-bound: $\textrm{O}(\textrm{f}(n)) = \textrm{g}(n)$, where $\textrm{g}(n)$ is the \textbf{highest} possible value of $\textrm{f(n)}$ as $n \rightarrow \infty$
  \item Lower-bound: $\Omega(\textrm{f}(n)) = \textrm{g}(n)$, where $\textrm{g}(n)$ is the \textbf{lowest} possible value of $\textrm{f(n)}$ as $n \rightarrow \infty$
  \item Average: $\Theta(\textrm{f}(n)) = \textrm{g}(n)$, where $\textrm{g}(n)$ is the \textbf{most likely} value of $\textrm{f(n)}$ as $n \rightarrow \infty$
\end{itemize}

Given

$$
\textrm{f}(n) = \log_{\ln{5}}{\left(\left(\log{n}\right)^{100}\right)}
$$

What is the time complexity? 

$$
\log{n^a} = a\cdot \log{n}
$$

Gives

$$
\textrm{f}(n) = 100\cdot\log_{\ln{5}}{\left(\log{n}\right)}
$$

Since the base does not matter in asymptotic complexity ($\textrm{lim}(\textrm{n}) \rightarrow \infty$), and neither does the constant $100$, the result is:

$$
\Theta(\textrm{f}{(n)}) = \log{\log{n}}
$$

\section{Models of Computation, Document Distance}

This lecture talks about different ways of reasoning about computation. It also introduces the concept of document distance, which can be used to determine the likeness of documents.

\begin{itemize}
  \item \href{https://www.youtube.com/watch?v=Zc54gFhdpLA}{Lecture}
\end{itemize}

\subsection{Models of Computation}

When reasoning about programs, there's a duality of described and actual. The design of algorithms does not concern itself with the actual, but rather with a description of a solution along with its most important characteristics. It is then up to the programmer to write a program, in a programming language, which runs on a computer.

To deal with how a computer actually works with the instructions that are part of an algorithm, something called a model of computation has been formed. These models allow algorithm designers to reason about how long it will take to perform an action, even if the actual hardware is unknown. This is very much similar to the lumped-circuit abstraction which ignores Maxwell's laws in favour of simple heuristics that work in the vast majority of cases.

There two most common models of computation are: Random Access Machine (RAM), and Pointer Machine. A RAM is visualized as one large chunk of memory consisting of cells. Each cell contains a word of data (typically 32 or 64 bits wide). A Pointer Machine deals with objects, and can point to other objects as needed. In a pointer machine, there is no need to consider the layout of memory.

\subsection{Document Distance}

A document is a sequence of words surrounded by whitespace and stopwords. A word is a sequence of alphanumerical symbols. The document distance is a measure of likeness between two documents.

One way to model words is to use a word vector. The most simple form of word vector is a frequency count of each word in the english language found in a document.

Comparing two documents is then a matter of comparing vectors.

One could imagine the use of dot-product (inner product). However, dot-products are not normalized, so the larger the text, the larger the dot-product.

A more efficient method is to use cosine-similarity - a measure of the angle between two vectors. This measure is given by

$$
  \cos{\alpha} = \frac{\textrm{D}_1\cdot\textrm{D}_2}{\left| \textrm{D}_1 \right| \left| \textrm{D}_1 \right|}
$$

This gives a normalized value from 0 to 1, depending on the similarity of the two vectors.

\end{document}